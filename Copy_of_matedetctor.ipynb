{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install chess\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rQTmUnE9-Ry",
        "outputId": "196877fc-615a-411b-cb8c-e452332e133b"
      },
      "id": "2rQTmUnE9-Ry",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chess in /usr/local/lib/python3.11/dist-packages (1.11.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchviz) (2.6.0+cu124)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n",
            "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fb599fe1",
      "metadata": {
        "id": "fb599fe1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import chess\n",
        "# Utility: convert FEN to tensor with positional features\n",
        "def fen_to_tensor(fen):\n",
        "    board = chess.Board(fen)\n",
        "    # Base piece planes: 12 channels\n",
        "    planes = np.zeros((12, 8, 8), dtype=np.float32)\n",
        "    for sq, piece in board.piece_map().items():\n",
        "        idx = {'P':0,'N':1,'B':2,'R':3,'Q':4,'K':5}[piece.symbol().upper()]\n",
        "        color_offset = 0 if piece.color == chess.WHITE else 6\n",
        "        row = 7 - (sq // 8)\n",
        "        col = sq % 8\n",
        "        planes[idx + color_offset, row, col] = 1\n",
        "\n",
        "    # Side to move plane\n",
        "    stm_plane = np.full((1, 8, 8), float(board.turn), dtype=np.float32)\n",
        "\n",
        "    # Additional positional features: 7 channels\n",
        "    # Attack maps (white, black)\n",
        "    attack_w = np.zeros((8, 8), dtype=np.float32)\n",
        "    attack_b = np.zeros((8, 8), dtype=np.float32)\n",
        "    for sq in chess.SQUARES:\n",
        "        r = 7 - (sq // 8)\n",
        "        c = sq % 8\n",
        "        if board.attackers(chess.WHITE, sq):\n",
        "            attack_w[r, c] = 1\n",
        "        if board.attackers(chess.BLACK, sq):\n",
        "            attack_b[r, c] = 1\n",
        "\n",
        "    # Legal move mask\n",
        "    legal_mask = np.zeros((8, 8), dtype=np.float32)\n",
        "    for mv in board.legal_moves:\n",
        "        r = 7 - (mv.to_square // 8)\n",
        "        c = mv.to_square % 8\n",
        "        legal_mask[r, c] = 1\n",
        "\n",
        "    # Distance to kings\n",
        "    dist_wk = np.zeros((8, 8), dtype=np.float32)\n",
        "    dist_bk = np.zeros((8, 8), dtype=np.float32)\n",
        "    wksq = board.king(chess.WHITE)\n",
        "    bksq = board.king(chess.BLACK)\n",
        "    for sq in chess.SQUARES:\n",
        "        r = 7 - (sq // 8)\n",
        "        c = sq % 8\n",
        "        if wksq is not None:\n",
        "            dist_wk[r, c] = chess.square_distance(sq, wksq)\n",
        "        if bksq is not None:\n",
        "            dist_bk[r, c] = chess.square_distance(sq, bksq)\n",
        "\n",
        "    # Check status plane\n",
        "    check_pl = np.full((8, 8), float(board.is_check()), dtype=np.float32)\n",
        "\n",
        "    # Pinned pieces map\n",
        "    pinned = np.zeros((8, 8), dtype=np.float32)\n",
        "    for sq in chess.SQUARES:\n",
        "        piece = board.piece_at(sq)\n",
        "        if piece and board.is_pinned(piece.color, sq):\n",
        "            r = 7 - (sq // 8)\n",
        "            c = sq % 8\n",
        "            pinned[r, c] = 1\n",
        "    # Checking moves mask\n",
        "    checking_moves_mask = np.zeros((8,8), dtype=np.float32)\n",
        "    for mv in board.legal_moves:\n",
        "        board.push(mv)\n",
        "        if board.is_check():\n",
        "            r = 7 - (mv.to_square // 8)\n",
        "            c = mv.to_square % 8\n",
        "            checking_moves_mask[r, c] = 1\n",
        "        board.pop()\n",
        "\n",
        "    controlled_white = np.zeros((8,8), dtype=np.float32)\n",
        "    controlled_black = np.zeros((8,8), dtype=np.float32)\n",
        "    for sq in chess.SQUARES:\n",
        "        r = 7 - (sq // 8)\n",
        "        c = sq % 8\n",
        "        white_attackers = len(board.attackers(chess.WHITE, sq))\n",
        "        black_attackers = len(board.attackers(chess.BLACK, sq))\n",
        "        if white_attackers > black_attackers:\n",
        "            controlled_white[r, c] = 1.0\n",
        "        elif black_attackers > white_attackers:\n",
        "            controlled_black[r, c] = 1.0\n",
        "\n",
        "     # Stack all planes: 12 + 1 + 2 + 1 + 2 + 1 + 1 + 2 = 22 channels\n",
        "    extra = [attack_w, attack_b, legal_mask, dist_wk, dist_bk, check_pl, pinned, controlled_white, controlled_black, checking_moves_mask]\n",
        "    feature_planes = np.stack(extra, axis=0)\n",
        "    all_planes = np.concatenate([planes, stm_plane, feature_planes], axis=0)\n",
        "\n",
        "    return torch.from_numpy(all_planes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5e10c450",
      "metadata": {
        "id": "5e10c450"
      },
      "outputs": [],
      "source": [
        "# --- Hyperparameters & Constants ---\n",
        "EPOCHS = 15          # Number of epochs to train\n",
        "BATCH_SIZE = 512          # Number of samples per batch\n",
        "MAX_BATCHES_PER_EPOCH = 10000  # Process exactly 600 batches per epoch\n",
        "LR = 1e-2                 # Learning rate\n",
        "BINARY_CLASSES = 2        # Number of classes for binary classification\n",
        "DATA_PATH = 'data/trainingpuzzles.csv'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Use more workers but be mindful of CPU limits\n",
        "train_loader = DataLoader(..., batch_size=32, num_workers=2, pin_memory=False)\n",
        "\n",
        "# Limit PyTorch CPU threads\n",
        "torch.set_num_threads(4)\n",
        "\n",
        "# Precompute fen tensors once outside of training\n",
        "# Save processed tensors to disk, then load quickly during training\n",
        "\n",
        "class NumberMateCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NumberMateCNN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(23, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(), nn.Linear(128*2*2, 256), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out = self.fc(self.conv(x)).squeeze(-1)\n",
        "        return torch.sigmoid(out)  # outputs in [0,1]\n",
        "\n",
        "# Binary classification model\n",
        "class IsMateCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IsMateCNN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(23, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(), nn.Linear(128*2*2, 256), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(256, BINARY_CLASSES)\n",
        "        )\n",
        "    def forward(self, x): return self.fc(self.conv(x))\n",
        "\n",
        "import json\n",
        "\n",
        "def train_binary(model, train_loader, val_loader, device, save_path='binary_model.pt', max_batches=MAX_BATCHES_PER_EPOCH):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "    model.to(device)\n",
        "\n",
        "    avg_train_losses = []\n",
        "    avg_val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"Training epoch {epoch}...\")\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "        samples_seen = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            if batch_count >= max_batches:\n",
        "                break\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            samples_seen += x.size(0)\n",
        "            if batch_count%25==0:\n",
        "                print(f\"Running batch {batch_count}\")\n",
        "            batch_count += 1\n",
        "\n",
        "        avg_train_loss = total_loss / samples_seen\n",
        "        avg_train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val in val_loader:\n",
        "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "                logits = model(x_val)\n",
        "                loss = criterion(logits, y_val)\n",
        "                val_loss += loss.item() * x_val.size(0)\n",
        "                preds = logits.argmax(dim=1)\n",
        "                correct += (preds == y_val).sum().item()\n",
        "                total += y_val.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss / total\n",
        "        val_accuracy = correct / total\n",
        "\n",
        "        avg_val_losses.append(avg_val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Train Loss: {avg_train_loss:.4f} Val Loss: {avg_val_loss:.4f} Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        torch.save(model.state_dict(), f\"{save_path}_epoch{epoch+1}.pt\")\n",
        "\n",
        "    # Save metrics to JSON file\n",
        "    with open(f\"{save_path}_metrics.json\", 'w') as f:\n",
        "        json.dump({\n",
        "            'train_loss': avg_train_losses,\n",
        "            'val_loss': avg_val_losses,\n",
        "            'val_accuracy': val_accuracies\n",
        "        }, f)\n",
        "\n",
        "def validate_distance(model, loader, device, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for x, t in loader:\n",
        "            x, t = x.to(device), t.to(device)\n",
        "            t_recip = torch.where(t > 0, 1.0 / t, torch.zeros_like(t))\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, t_recip)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            total_samples += x.size(0)\n",
        "    avg_loss = total_loss / total_samples\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_distance(model, train_loader, val_loader, device, save_path='distance_model.pt'):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "    model.to(device)\n",
        "\n",
        "    EPOCHS = 30\n",
        "    error_threshold = 1  # acceptable error threshold in mate moves\n",
        "\n",
        "    avg_train_losses = []\n",
        "    avg_val_losses = []\n",
        "    val_accuracies = []\n",
        "    val_mean_abs_errors = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"Training epoch {epoch}...\")\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for x, t in train_loader:\n",
        "            x, t = x.to(device), t.to(device)\n",
        "            # target: inverse mate distance (1/mate_moves), 0 if no mate\n",
        "            t_recip = torch.where(t > 0, 1.0 / t, torch.zeros_like(t))\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, t_recip)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item() * x.size(0)\n",
        "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
        "        avg_train_losses.append(avg_train_loss)\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        abs_error_sum = 0\n",
        "        correct_within_threshold = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_val, t_val in val_loader:\n",
        "                x_val, t_val = x_val.to(device), t_val.to(device)\n",
        "                t_recip_val = torch.where(t_val > 0, 1.0 / t_val, torch.zeros_like(t_val))\n",
        "                pred_val = model(x_val)\n",
        "\n",
        "                loss_val = criterion(pred_val, t_recip_val)\n",
        "                val_loss += loss_val.item() * x_val.size(0)\n",
        "\n",
        "                # Convert predictions back to mate moves\n",
        "                pred_moves = torch.where(pred_val > 0, 1.0 / pred_val, torch.full_like(pred_val, 1000.0))\n",
        "                abs_move_errors = torch.abs(pred_moves - t_val)\n",
        "\n",
        "                abs_error_sum += abs_move_errors.sum().item()\n",
        "                correct_within_threshold += (abs_move_errors <= error_threshold).sum().item()\n",
        "                total += t_val.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "        avg_val_losses.append(avg_val_loss)\n",
        "\n",
        "        mean_abs_error = abs_error_sum / total\n",
        "        val_mean_abs_errors.append(mean_abs_error)\n",
        "\n",
        "        val_accuracy = correct_within_threshold / total\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_accuracy:.4f}, Mean Abs Error={mean_abs_error:.4f} moves\")\n",
        "\n",
        "        torch.save(model.state_dict(), f\"{save_path}_epoch{epoch+1}.pt\")\n",
        "\n",
        "    # Save metrics to JSON\n",
        "    with open(f\"{save_path}_metrics.json\", 'w') as f:\n",
        "        json.dump({\n",
        "            'train_loss': avg_train_losses,\n",
        "            'val_loss': avg_val_losses,\n",
        "            'val_accuracy': val_accuracies,\n",
        "            'val_mean_abs_error': val_mean_abs_errors\n",
        "        }, f)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: ok load the model from binary_model_epoch16.pt\n",
        "\n",
        "# Instantiate the model\n",
        "model = IsMateCNN() # Assuming you want to load the IsMateCNN model based on the filename\n",
        "\n",
        "# Load the saved state dictionary\n",
        "# Make sure the file 'binary_model_epoch16.pt' is in your Colab environment or mounted Drive\n",
        "model.load_state_dict(torch.load('binary_model_epoch16.pt', map_location=DEVICE))\n",
        "\n",
        "# Move the model to the appropriate device (CPU or GPU)\n",
        "model.to(DEVICE)\n",
        "\n",
        "# Set the model to evaluation mode (important for inference)\n",
        "model.eval()\n",
        "sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "4PqgZky93kHy",
        "outputId": "9c886a84-9db5-4391-aeb9-d596931a5235"
      },
      "id": "4PqgZky93kHy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'binary_model_epoch16.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2e9d4fae5380>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the saved state dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Make sure the file 'binary_model_epoch16.pt' is in your Colab environment or mounted Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'binary_model_epoch16.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Move the model to the appropriate device (CPU or GPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'binary_model_epoch16.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f8afeef",
      "metadata": {
        "id": "1f8afeef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a8d5aa-2fbe-46e1-ecef-e08e6071e9cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-46e4069cc2fc>:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.sample(min_class_size, replace=False))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced classes count: {0: 270000, 1: 270000}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# --- Constants ---\n",
        "EPOCHS = 4\n",
        "BATCH_SIZE = 512*2\n",
        "LR = 1e-3\n",
        "BINARY_CLASSES = 2\n",
        "DATA_PATH = 'data/trainingpuzzles.csv'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class ChessPuzzleBinaryDataset(Dataset):\n",
        "    def __init__(self, csv_file, task='binary'):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "\n",
        "        self.task = task  # 'binary' or 'distance'\n",
        "\n",
        "        if self.task == 'binary':\n",
        "            # Create binary label: 0 if number==0, else 1\n",
        "            self.df['distance'] = self.df['Number'].astype(int)\n",
        "            self.df['label'] = (self.df['Number']!=0).astype(int)\n",
        "\n",
        "            # Balance classes by downsampling to the smallest class size\n",
        "            min_class_size = self.df['label'].value_counts().min()\n",
        "            self.df = (\n",
        "                self.df.groupby('label')\n",
        "                .apply(lambda x: x.sample(min_class_size, replace=False))\n",
        "                .reset_index(drop=True)\n",
        "            )\n",
        "            print(f\"Balanced classes count: {self.df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        fen = row['FEN']\n",
        "        moves = row['Moves']\n",
        "        number = int(row['Number'])\n",
        "        x = fen_to_tensor(fen)  # You provide this function\n",
        "\n",
        "        if self.task == 'binary':\n",
        "            y = torch.tensor(row['label'], dtype=torch.long)\n",
        "        else:\n",
        "            # Distance regression target\n",
        "            try:\n",
        "                mate_dist = number\n",
        "            except:\n",
        "                mate_dist = 0\n",
        "            y = torch.tensor(mate_dist, dtype=torch.float32)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "# --- Load and split dataset ---\n",
        "full_ds = ChessPuzzleBinaryDataset(DATA_PATH, task='binary')\n",
        "\n",
        "labels = full_ds.df['label'].values  # stratify needs labels array\n",
        "indices = list(range(len(full_ds)))\n",
        "\n",
        "train_idx, val_idx = train_test_split(\n",
        "    indices,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(Subset(full_ds, train_idx), batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader   = DataLoader(Subset(full_ds, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "# Assuming val_loader.dataset is accessible and has targets attribute or __getitem__ returns (x, y)\n",
        "val_labels = []\n",
        "\n",
        "for _, y in val_loader:\n",
        "    val_labels.extend(y.cpu().numpy())\n",
        "\n",
        "print(\"Validation label distribution:\", Counter(val_labels))\n",
        "\n",
        "# --- Train binary classification model ---\n",
        "binary_model = IsMateCNN()\n",
        "train_binary(binary_model, train_loader, val_loader, DEVICE, save_path='binary_model')\n",
        "\n",
        "# --- Switch dataset mode to distance regression ---\n",
        "full_ds.task = 'distance'\n",
        "train_loader = DataLoader(Subset(full_ds, train_idx), batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "# --- Train mate-distance regression model ---\n",
        "distance_model = NumberMateCNN()\n",
        "train_distance(distance_model, train_loader, val_loader, DEVICE, save_path='distance_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume fen_to_tensor(fen) is defined elsewhere and converts a FEN string to a (C,H,W) tensor\n",
        "\n",
        "def flip_fen_horizontal(fen):\n",
        "    # This function flips the FEN horizontally.\n",
        "    # Implement or use a chess library to do this properly.\n",
        "    # Here is a simple placeholder (you need to replace it with a correct one).\n",
        "    # The core idea: flip each rank string horizontally.\n",
        "    parts = fen.split(' ')\n",
        "    board = parts[0]\n",
        "    ranks = board.split('/')\n",
        "    flipped_ranks = []\n",
        "    for rank in ranks:\n",
        "        flipped_rank = ''\n",
        "        for ch in reversed(rank):\n",
        "            flipped_rank += ch\n",
        "        flipped_ranks.append(flipped_rank)\n",
        "    flipped_board = '/'.join(flipped_ranks)\n",
        "    parts[0] = flipped_board\n",
        "    return ' '.join(parts)\n",
        "\n",
        "class ChessPuzzleBinaryDataset(Dataset):\n",
        "    def __init__(self, csv_file, task='binary'):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "\n",
        "        self.task = task\n",
        "\n",
        "        if self.task == 'binary':\n",
        "            self.df['label'] = (self.df['Number'] != 0).astype(int)\n",
        "\n",
        "            # Balance classes by downsampling to the smallest class size\n",
        "            min_class_size = self.df['label'].value_counts().min()\n",
        "            self.df = (\n",
        "                self.df.groupby('label')\n",
        "                .apply(lambda x: x.sample(min_class_size, replace=False))\n",
        "                .reset_index(drop=True)\n",
        "            )\n",
        "            print(f\"Balanced classes count: {self.df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "            # Create flipped versions explicitly\n",
        "            flipped_df = self.df.copy()\n",
        "            flipped_df['FEN'] = flipped_df['FEN'].apply(flip_fen_horizontal)\n",
        "            # The label stays the same for flipped boards\n",
        "            self.df = pd.concat([self.df, flipped_df], ignore_index=True)\n",
        "            print(f\"Dataset size after adding flipped: {len(self.df)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        fen = row['FEN']\n",
        "        y = torch.tensor(row['label'], dtype=torch.long) if self.task == 'binary' else torch.tensor(row['Number'], dtype=torch.float32)\n",
        "        x = fen_to_tensor(fen)  # Your function to convert FEN to tensor\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def train_binary(model, train_loader, val_loader, device, save_path='binary_model', start_epoch=0, max_batches=None):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "    model.to(device)\n",
        "\n",
        "    if start_epoch > 0:\n",
        "        load_path = f\"{save_path}_epoch{start_epoch}.pt\"\n",
        "        if os.path.exists(load_path):\n",
        "            print(f\"Loading model state from {load_path}\")\n",
        "            model.load_state_dict(torch.load(load_path))\n",
        "        else:\n",
        "            print(f\"Warning: State dict not found at {load_path}. Starting fresh.\")\n",
        "\n",
        "    avg_train_losses = []\n",
        "    avg_val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(start_epoch, start_epoch + EPOCHS):\n",
        "        print(f\"Training epoch {epoch}...\")\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        samples_seen = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            if max_batches and batch_count >= max_batches:\n",
        "                break\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            samples_seen += x.size(0)\n",
        "\n",
        "            if batch_count % 25 == 0:\n",
        "                print(f\"Running batch {batch_count}\")\n",
        "            batch_count += 1\n",
        "\n",
        "        avg_train_loss = total_loss / samples_seen\n",
        "        avg_train_losses.append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val in val_loader:\n",
        "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "                logits = model(x_val)\n",
        "                loss = criterion(logits, y_val)\n",
        "                val_loss += loss.item() * x_val.size(0)\n",
        "                preds = logits.argmax(dim=1)\n",
        "                correct += (preds == y_val).sum().item()\n",
        "                total += y_val.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss / total\n",
        "        val_accuracy = correct / total\n",
        "        avg_val_losses.append(avg_val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Train Loss: {avg_train_loss:.4f} Val Loss: {avg_val_loss:.4f} Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        torch.save(model.state_dict(), f\"{save_path}_epoch{epoch+1}.pt\")\n",
        "\n",
        "    # Save metrics, merge if resuming\n",
        "    metrics_file = f\"{save_path}_metrics.json\"\n",
        "    if os.path.exists(metrics_file) and start_epoch > 0:\n",
        "        with open(metrics_file, 'r') as f:\n",
        "            existing = json.load(f)\n",
        "        avg_train_losses = existing.get('train_loss', []) + avg_train_losses\n",
        "        avg_val_losses = existing.get('val_loss', []) + avg_val_losses\n",
        "        val_accuracies = existing.get('val_accuracy', []) + val_accuracies\n",
        "\n",
        "    with open(metrics_file, 'w') as f:\n",
        "        json.dump({\n",
        "            'train_loss': avg_train_losses,\n",
        "            'val_loss': avg_val_losses,\n",
        "            'val_accuracy': val_accuracies\n",
        "        }, f)\n",
        "\n",
        "\n",
        "def train_distance(model, train_loader, val_loader, device, save_path='distance_model', start_epoch=0):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    model.to(device)\n",
        "\n",
        "    if start_epoch > 0:\n",
        "        load_path = f\"{save_path}_epoch{start_epoch}.pt\"\n",
        "        if os.path.exists(load_path):\n",
        "            print(f\"Loading model state from {load_path}\")\n",
        "            model.load_state_dict(torch.load(load_path))\n",
        "        else:\n",
        "            print(f\"Warning: State dict not found at {load_path}. Starting fresh.\")\n",
        "\n",
        "    avg_train_losses = []\n",
        "    avg_val_losses = []\n",
        "    val_accuracies = []\n",
        "    val_mean_abs_errors = []\n",
        "\n",
        "    error_threshold = 1  # moves\n",
        "\n",
        "    for epoch in range(start_epoch, start_epoch + EPOCHS):\n",
        "        print(f\"Training epoch {epoch}...\")\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        samples_seen = 0\n",
        "\n",
        "        for x, t in train_loader:\n",
        "            x, t = x.to(device), t.to(device)\n",
        "            t_recip = torch.where(t > 0, 1.0 / t, torch.zeros_like(t))\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, t_recip)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item() * x.size(0)\n",
        "            samples_seen += x.size(0)\n",
        "\n",
        "        avg_train_loss = total_train_loss / samples_seen\n",
        "        avg_train_losses.append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        abs_error_sum = 0\n",
        "        correct_within_threshold = 0\n",
        "        total_val_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_val, t_val in val_loader:\n",
        "                x_val, t_val = x_val.to(device), t_val.to(device)\n",
        "                t_recip_val = torch.where(t_val > 0, 1.0 / t_val, torch.zeros_like(t_val))\n",
        "                pred_val = model(x_val)\n",
        "                loss_val = criterion(pred_val, t_recip_val)\n",
        "                val_loss += loss_val.item() * x_val.size(0)\n",
        "\n",
        "                pred_moves = torch.where(pred_val > 1e-6, 1.0 / pred_val, torch.full_like(pred_val, 1000.0))\n",
        "                abs_move_errors = torch.abs(pred_moves - t_val)\n",
        "\n",
        "                abs_error_sum += abs_move_errors.sum().item()\n",
        "                correct_within_threshold += (abs_move_errors <= error_threshold).sum().item()\n",
        "                total_val_samples += t_val.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss / total_val_samples\n",
        "        mean_abs_error = abs_error_sum / total_val_samples\n",
        "        val_accuracy = correct_within_threshold / total_val_samples\n",
        "\n",
        "        avg_val_losses.append(avg_val_loss)\n",
        "        val_mean_abs_errors.append(mean_abs_error)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_accuracy:.4f}, Mean Abs Error={mean_abs_error:.4f} moves\")\n",
        "\n",
        "        torch.save(model.state_dict(), f\"{save_path}_epoch{epoch+1}.pt\")\n",
        "\n",
        "    metrics_file = f\"{save_path}_metrics.json\"\n",
        "    if os.path.exists(metrics_file) and start_epoch > 0:\n",
        "        with open(metrics_file, 'r') as f:\n",
        "            existing = json.load(f)\n",
        "        avg_train_losses = existing.get('train_loss', []) + avg_train_losses\n",
        "        avg_val_losses = existing.get('val_loss', []) + avg_val_losses\n",
        "        val_accuracies = existing.get('val_accuracy', []) + val_accuracies\n",
        "        val_mean_abs_errors = existing.get('val_mean_abs_error', []) + val_mean_abs_errors\n",
        "\n",
        "    with open(metrics_file, 'w') as f:\n",
        "        json.dump({\n",
        "            'train_loss': avg_train_losses,\n",
        "            'val_loss': avg_val_losses,\n",
        "            'val_accuracy': val_accuracies,\n",
        "            'val_mean_abs_error': val_mean_abs_errors\n",
        "        }, f)\n",
        "\n",
        "\n",
        "# --- Constants ---\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 512\n",
        "LR = 1e-4\n",
        "DATA_PATH = 'data/trainingpuzzles.csv'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "START_EPOCH_BINARY = 4\n",
        "START_EPOCH_DISTANCE = 4\n",
        "\n",
        "# --- Load dataset ---\n",
        "full_ds = ChessPuzzleBinaryDataset(DATA_PATH, task='binary')\n",
        "\n",
        "# Use actual length of balanced + flipped dataset for splitting\n",
        "dataset_len = len(full_ds)\n",
        "print(f\"Total balanced dataset size (including flipped): {dataset_len}\")\n",
        "\n",
        "indices = list(range(dataset_len))\n",
        "train_idx, val_idx = train_test_split(indices, test_size=0.1, random_state=42)\n",
        "\n",
        "train_loader_binary = DataLoader(\n",
        "    Subset(full_ds, train_idx),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader_binary = DataLoader(\n",
        "    Subset(full_ds, val_idx),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# --- Your model classes should be defined somewhere ---\n",
        "binary_model = IsMateCNN()\n",
        "\n",
        "# Train binary model from saved epoch 4\n",
        "train_binary(binary_model, train_loader_binary, val_loader_binary, DEVICE, save_path='binary_model', start_epoch=START_EPOCH_BINARY)\n"
      ],
      "metadata": {
        "id": "BpHp6ZZ1lcgm"
      },
      "id": "BpHp6ZZ1lcgm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = IsMateCNN()\n",
        "model.load_state_dict(torch.load('binary_model_epoch16.pt', map_location=torch.device('cpu')))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36s1UwrDPkfY",
        "outputId": "d506b873-5e9b-4b56-e4a2-b4ff67f5b5cf"
      },
      "id": "36s1UwrDPkfY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IsMateCNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(23, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "class ChessPuzzleBinaryDatasetUnbalanced(Dataset):\n",
        "    def __init__(self, csv_file, task='binary'):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "\n",
        "        self.task = task  # 'binary' or 'distance'\n",
        "\n",
        "        if self.task == 'binary':\n",
        "            # Create binary label: 0 if number==0, else 1\n",
        "            self.df['distance'] = self.df['Number']\n",
        "            self.df['label'] = (self.df['Number'] != 0).astype(int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        fen = row['FEN']\n",
        "        moves = row['Moves']\n",
        "        number = int(row['Number'])\n",
        "        x = fen_to_tensor(fen)  # You provide this function\n",
        "\n",
        "        if self.task == 'binary':\n",
        "            y = torch.tensor(row['label'], dtype=torch.long)\n",
        "        else:\n",
        "            # Distance regression target\n",
        "            try:\n",
        "                mate_dist = number\n",
        "            except:\n",
        "                mate_dist = 0\n",
        "            y = torch.tensor(mate_dist, dtype=torch.float32)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "test_ds = ChessPuzzleBinaryDatasetUnbalanced('testingpuzzles.csv', task='binary')\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "fFKMvF0XRyhw"
      },
      "id": "fFKMvF0XRyhw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds = []\n",
        "all_labels = []\n",
        "print(f\"Total samples in test dataset: {len(test_ds)}\")\n",
        "total_processed = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)  # use your loaded model here\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_labels.append(y.cpu())\n",
        "\n",
        "        total_processed += y.size(0)\n",
        "        if total_processed % 5000 < y.size(0):\n",
        "            print(f\"Processed {total_processed} puzzles so far...\")\n",
        "            print(f\"Total predictions collected: {sum(p.size(0) for p in all_preds)}\")\n",
        "\n",
        "all_preds = torch.cat(all_preds)\n",
        "all_labels = torch.cat(all_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rgwdaywR4_6",
        "outputId": "147238d7-e7b1-42fc-ded3-8c04d521802f"
      },
      "id": "4rgwdaywR4_6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples in test dataset: 171444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 5120 puzzles so far...\n",
            "Total predictions collected: 5120\n",
            "Processed 10240 puzzles so far...\n",
            "Total predictions collected: 10240\n",
            "Processed 15360 puzzles so far...\n",
            "Total predictions collected: 15360\n",
            "Processed 20480 puzzles so far...\n",
            "Total predictions collected: 20480\n",
            "Processed 25088 puzzles so far...\n",
            "Total predictions collected: 25088\n",
            "Processed 30208 puzzles so far...\n",
            "Total predictions collected: 30208\n",
            "Processed 35328 puzzles so far...\n",
            "Total predictions collected: 35328\n",
            "Processed 40448 puzzles so far...\n",
            "Total predictions collected: 40448\n",
            "Processed 45056 puzzles so far...\n",
            "Total predictions collected: 45056\n",
            "Processed 50176 puzzles so far...\n",
            "Total predictions collected: 50176\n",
            "Processed 55296 puzzles so far...\n",
            "Total predictions collected: 55296\n",
            "Processed 60416 puzzles so far...\n",
            "Total predictions collected: 60416\n",
            "Processed 65024 puzzles so far...\n",
            "Total predictions collected: 65024\n",
            "Processed 70144 puzzles so far...\n",
            "Total predictions collected: 70144\n",
            "Processed 75264 puzzles so far...\n",
            "Total predictions collected: 75264\n",
            "Processed 80384 puzzles so far...\n",
            "Total predictions collected: 80384\n",
            "Processed 85504 puzzles so far...\n",
            "Total predictions collected: 85504\n",
            "Processed 90112 puzzles so far...\n",
            "Total predictions collected: 90112\n",
            "Processed 95232 puzzles so far...\n",
            "Total predictions collected: 95232\n",
            "Processed 100352 puzzles so far...\n",
            "Total predictions collected: 100352\n",
            "Processed 105472 puzzles so far...\n",
            "Total predictions collected: 105472\n",
            "Processed 110080 puzzles so far...\n",
            "Total predictions collected: 110080\n",
            "Processed 115200 puzzles so far...\n",
            "Total predictions collected: 115200\n",
            "Processed 120320 puzzles so far...\n",
            "Total predictions collected: 120320\n",
            "Processed 125440 puzzles so far...\n",
            "Total predictions collected: 125440\n",
            "Processed 130048 puzzles so far...\n",
            "Total predictions collected: 130048\n",
            "Processed 135168 puzzles so far...\n",
            "Total predictions collected: 135168\n",
            "Processed 140288 puzzles so far...\n",
            "Total predictions collected: 140288\n",
            "Processed 145408 puzzles so far...\n",
            "Total predictions collected: 145408\n",
            "Processed 150016 puzzles so far...\n",
            "Total predictions collected: 150016\n",
            "Processed 155136 puzzles so far...\n",
            "Total predictions collected: 155136\n",
            "Processed 160256 puzzles so far...\n",
            "Total predictions collected: 160256\n",
            "Processed 165376 puzzles so far...\n",
            "Total predictions collected: 165376\n",
            "Processed 170496 puzzles so far...\n",
            "Total predictions collected: 170496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_vs_Ut1Ks3Ir"
      },
      "id": "_vs_Ut1Ks3Ir",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming all_labels and all_preds are 1D numpy arrays or tensors\n",
        "\n",
        "# Convert to numpy arrays if they are PyTorch tensors\n",
        "y_true = all_labels.cpu().numpy() if isinstance(all_labels, torch.Tensor) else all_labels\n",
        "y_pred = all_preds.cpu().numpy() if isinstance(all_preds, torch.Tensor) else all_preds\n",
        "\n",
        "# Calculate the confusion matrix components manually\n",
        "# True Negatives (TN): Actual 0, Predicted 0 (Correctly predicted No Mate)\n",
        "tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "\n",
        "# False Positives (FP): Actual 0, Predicted 1 (Incorrectly predicted Mate when it was No Mate)\n",
        "fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "\n",
        "# False Negatives (FN): Actual 1, Predicted 0 (Incorrectly predicted No Mate when it was Mate)\n",
        "fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "# True Positives (TP): Actual 1, Predicted 1 (Correctly predicted Mate)\n",
        "tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "\n",
        "# Print the confusion matrix in a text format\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"-------------------------\")\n",
        "print(f\"{'':<15} | {'Predicted No Mate':<15} | {'Predicted Mate':<15}\")\n",
        "print(\"-------------------------\")\n",
        "print(f\"{'Actual No Mate':<15} | {tn:<15} | {fp:<15}\")\n",
        "print(f\"{'Actual Mate':<15} | {fn:<15} | {tp:<15}\")\n",
        "print(\"-------------------------\")\n",
        "\n",
        "# Optional: Print summary statistics\n",
        "total_samples = len(y_true)\n",
        "accuracy = (tp + tn) / total_samples\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0 # Avoid division by zero\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0   # Avoid division by zero\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0 # Avoid division by zero\n",
        "\n",
        "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (for Mate): {precision:.4f}\")\n",
        "print(f\"Recall (for Mate): {recall:.4f}\")\n",
        "print(f\"F1-Score (for Mate): {f1_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6x2vHFLhNPf",
        "outputId": "fae79a93-8689-4b94-e09d-4d575c3f2f3a"
      },
      "id": "Q6x2vHFLhNPf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "-------------------------\n",
            "                | Predicted No Mate | Predicted Mate \n",
            "-------------------------\n",
            "Actual No Mate  | 24565           | 5435           \n",
            "Actual Mate     | 24753           | 116691         \n",
            "-------------------------\n",
            "\n",
            "Overall Accuracy: 0.8239\n",
            "Precision (for Mate): 0.9555\n",
            "Recall (for Mate): 0.8250\n",
            "F1-Score (for Mate): 0.8855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure these are 1D arrays of 0s and 1s\n",
        "y_true = all_labels.cpu().numpy() if isinstance(all_labels, torch.Tensor) else all_labels\n",
        "y_pred = all_preds.cpu().numpy() if isinstance(all_preds, torch.Tensor) else all_preds\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "\n",
        "# Optional: Normalize by row to get % accuracy per class\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Display the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Mate', 'Mate'])\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title(\"Confusion Matrix (Mate / No Mate)\")\n",
        "plt.show()\n",
        "\n",
        "# Optional: show normalized percentages\n",
        "disp_norm = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=['No Mate', 'Mate'])\n",
        "disp_norm.plot(cmap='Blues', values_format=\".2f\")\n",
        "plt.title(\"Normalized Confusion Matrix (Mate / No Mate)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "m0EqgJcUiBAH"
      },
      "id": "m0EqgJcUiBAH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"CSV rows: {len(df_test)}\")\n",
        "print(f\"Dataset size: {len(test_loader.dataset)}\")\n",
        "print(f\"All preds length: {len(all_preds)}\")\n",
        "print(f\"All labels length: {len(all_labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBKcsQr2cir1",
        "outputId": "c538ed96-5414-4ea8-8e7f-792934c6e5c2"
      },
      "id": "xBKcsQr2cir1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV rows: 171444\n",
            "Dataset size: 60000\n",
            "All preds length: 36\n",
            "All labels length: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "# Instantiate your model\n",
        "model = IsMateCNN()\n",
        "# Your fen_to_tensor function outputs shape (C, H, W) = (23, 8, 8)\n",
        "dummy_input = torch.randn(1, 23, 8, 8)\n",
        "# Generate the graph\n",
        "dot = make_dot(model(dummy_input), params=dict(model.named_parameters()))\n",
        "dot.format = 'png'\n",
        "dot.render('is_mate_cnn_graph', cleanup=True) # This saves a file named is_mate_cnn_graph.png\n",
        "# You can then view the 'is_mate_cnn_graph.png' file in your Colab files pane."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "isNd3Bf8Xkce",
        "outputId": "356c2f3a-491b-4474-fd59-bfa265dae666"
      },
      "id": "isNd3Bf8Xkce",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is_mate_cnn_graph.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your CSV\n",
        "df = pd.read_csv('testingpuzzles.csv')\n",
        "\n",
        "# Count occurrences of each unique number in 'Number' column\n",
        "counts = df['Number'].value_counts().sort_index()\n",
        "\n",
        "# Calculate percentages\n",
        "percentages = counts / counts.sum() * 100\n",
        "\n",
        "# Display nicely\n",
        "for number, percent in percentages.items():\n",
        "    print(f\"Number {number}: {percent:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoNN8FvrUHTw",
        "outputId": "d376e32c-bc1f-4765-b525-7e050d7b9774"
      },
      "id": "ZoNN8FvrUHTw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number 0: 17.50%\n",
            "Number 1: 36.60%\n",
            "Number 2: 35.69%\n",
            "Number 3: 8.68%\n",
            "Number 4: 1.26%\n",
            "Number 5: 0.27%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}